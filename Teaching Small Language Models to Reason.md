# Teaching Small Language Models to Reason

## 问题和背景

cot的推理能力只有在百亿级别的模型上才涌现出来，而较小的LM的推理能力并没有随着CoT的提示而提高，大多小模型都产生了不合逻辑的CoT。而且，CoT提示甚至降低了参数少于100亿的模型的准确性。因此在本文中，作者探索了通过知识蒸馏将这种推理能力转移到更小的模型中，并研究了模型和数据集大小的权衡。

## 本文方法

具体来说，作者在一个更大的教师模型产生的思想输出链上微调一个小的学生模型，提出了一个用于 CoT 知识蒸馏的两步pipeline方法。

第一步包括使用教师模型生成的CoT推理对现有的监督数据集进行标注，在实验中作者使用了PaLM-540B和GPT-175B来对标数据集进行标注。作者在few shot prompt的设置下让LLM针对问题产生推理路径，需要注意的是，作者提到在提出一个问题之后再进行插入few shot prompt并解释说这样可以让模型纠正一些在COT上的错误；

第二步就是让小模型在有COT标注的数据下采用教师强制的方式进行微调，即给定问题，让小模型尝试去生成reasoning path和answer；而对小模型的训练则不需要prompt；

<div align=center>
<img src=assets/image-20231013133448-yp58lcv.png height=440 width=400/>
</div>


## 实验和分析

作者在算数，常识和符号推理三个任务上进行了实验，使用的数据集分别为GSM8K，MAWPS和ASDiv；

<div align=center>
<img src=assets/image-20231013140829-hi0q48g.png height=275 width=400/>
</div>

需要注意的是，作者在涉及计算的数据中都再用了外部的计算器来纠正模型生成的结果，即在多步运算中，比如“5+5 = 11，11*2=22“，则作者会采用外部的计算器来纠正第一步的结果”5+5=10“，然后在后续的过程中利用正确结果"10"来替换所有不正确的结果"11"，**相当于是尽管模型给出了正确的运算表达式但是却算不对结果，所以人为介入去计算正确结果？可是介入的小模型还是大模型呢？**

可以看到在采用了大模型产生的CoT注释的数据进行微调的小模型t5-xxl有很明显的提升，作者还研究了不同模型尺度，不同数据量以及原始数据对结果的影响：

<div align=center>
<img src=assets/image-20231013143150-xg4vqnk.png height=275 width=400/>
</div>

originCot来自原始的数据标注而其他情况来自大模型生成，值得注意的是在StrategyQA上的采用原始COT标注比模型生成的效果要好，作者分析由模型生成的数据可能存在事实错误；


<div align=center>
<img src=assets/image-20231013143555-vtsfmse.png height=375 width=500/>
<img src=assets/image-20231013143630-5hnz6nk.png height=250 width=400/>
</div>

## 总结和分析

1. 作者采用llm生成的思维链数据来对小模型进行微调，改进了小模型的效果，此外还在模型生成过程中来引入外部计算器来纠正结果。
2. 本文并没有解决语言模型无法处理运算表达式的问题，实际上语言模型并不理解一个实际的运算表达式，所以在引入了外部计算器的情况下效果改进才会这么大。